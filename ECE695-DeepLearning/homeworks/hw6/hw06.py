# -*- coding: utf-8 -*-
"""hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KBqAXcdx_3TZYFwohrfV3Xyr-QBucqLK

# ECE695 DL HW6: RNNs
Author: Rahul Deshmukh\
email: deshmuk5@purdue.edu

References:
  1. DLstudio: https://engineering.purdue.edu/kak/distDLS/DLStudio-1.1.3.html
  2. LSTMs:
    * Regular LSTM cell: https://pytorch.org/docs/stable/nn.html#lstmcell
    * Understanding LSTMs and its variants: https://colah.github.io/posts/2015-08-Understanding-LSTMs/

Description of what I have tried for this homework:
  1. Task-1: Experimented with integer representation as word encoding with GRUnet as the model. Noticed that integer representation works faster however, there was no appreciabble change in the performance of the model. One important observation was that we need to normalize the integer representation vector before sending it to the network. w/o normalization my loss was going to nan and inf.
  2. Task-2: Modified TEXTnetOrder2 to manually carry out LSTM cell with peephole modifications for gating functions. 
  3. Task-3: Tried batch learning by padding review to max review length. While padding the reviews, I first added an empty string ('') into the vocab and then padded the review tensor with integer index corresponding to the empty string. This way, the padding will not be biased towards any word in the original vocab. I used my TEXTnetOrder2_mod model for this task. I chose a batch size of 8 for this task and thus the number of iters reported in output.txt is smaller than Task-1/2.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import random
import numpy as np
import matplotlib.pyplot as plt
import torch
import os, sys

from google.colab import drive
drive.mount('/content/gdrive')

seed = 0           
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
torch.backends.cudnn.deterministic=True
torch.backends.cudnn.benchmarks=False
os.environ['PYTHONHASHSEED'] = str(seed)

"""##(0) Install DLStudio 1.1.3"""

# Commented out IPython magic to ensure Python compatibility.
dls_path = './gdrive/My Drive/ECE695_DL/HW56/DLStudio-1.1.3/'
data_base_path = './gdrive/My Drive/ECE695_DL/HW6/data/'
# %cd ./gdrive/My\ Drive/ECE695_DL/HW6/DLStudio-1.1.3/
# !sudo python3 setup.py install

"""## (0.1) Running DLStudio scripts

no GRU script
"""

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_no_gru.py 3 0

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_no_gru.py 40 0

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_no_gru.py 200 0

"""`TEXTnetOrder2` model without GRU script"""

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_no_gru.py 3 1

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_no_gru.py 40 1

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_no_gru.py 200 1

"""GRU script"""

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_with_gru.py 3

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_with_gru.py 40

# Commented out IPython magic to ensure Python compatibility.
# %run ./Examples/text_classification_with_gru.py 200

"""#(1) Task-1: Alternatives to modeling text with one-hot vectors
  * Representing text with numbers, using integer representation of the index of the word int he vocabulary
  * Using word2vec representations
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import numpy as np
import random
import gzip, pickle
import os,sys

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

if torch.cuda.is_available():
  device = torch.device("cuda:0")
else:
  device = torch.device("cpu")

"""###(1.1) Representing text with numbers, using integer representation of the index of the word in the vocabulary"""

# Dataset Definition
class Word_as_Int_Dataset(Dataset):
  root = './gdrive/My Drive/ECE695_DL/HW6/data/'
  def __init__(self,dataset_size,train=True, pad = False):
    super().__init__()    
    self.train= train
    self.dataset_size = dataset_size
    self.pad = pad
    if train:
      dataset_name = self.root+'sentiment_dataset_train_'+dataset_size+'.tar.gz'
    else:
      dataset_name = self.root+'sentiment_dataset_test_'+dataset_size+'.tar.gz'
    f = gzip.open(dataset_name, 'rb')
    dataset = f.read()
    if sys.version_info[0] == 3:
      self.positive_reviews, self.negative_reviews, self.vocab = pickle.loads(dataset, encoding='latin1')
    else:
      self.positive_reviews, self.negative_reviews, self.vocab = pickle.loads(dataset)
    self.vocab = sorted(self.vocab)
    self.categories = sorted(list(self.positive_reviews.keys()))
    self.category_sizes_pos = {category : len(self.positive_reviews[category]) for category in self.categories}
    self.category_sizes_neg = {category : len(self.negative_reviews[category]) for category in self.categories}
    self.indexed_dataset = []
    
    for category in self.positive_reviews:
        for review in self.positive_reviews[category]:
            self.indexed_dataset.append([review, category, 1])
    for category in self.negative_reviews:
        for review in self.negative_reviews[category]:
            self.indexed_dataset.append([review, category, 0])
    random.shuffle(self.indexed_dataset)
    if pad: 
      self.vocab.append('') # add empty string as the padding char
      _, self.max_len = self._corpus_stats(self.indexed_dataset)

  def __len__(self):
    return len(self.indexed_dataset)

  def __getitem__(self, idx):
    sample = self.indexed_dataset[idx] 
    review = sample[0]
    review_category = sample[1]
    review_sentiment = sample[2]
    review_sentiment = self._sentiment_to_tensor(review_sentiment)
    review_tensor = self._review_to_int_tensor(review)
    category_index = self.categories.index(review_category)
    sample = {'review'       : review_tensor, 
              'category'     : category_index, # should be converted to tensor, but not yet used
              'sentiment'    : review_sentiment }
    return sample

  def _review_to_int_tensor(self, review):
    if not self.pad:
      review_tensor = torch.zeros(len(review))
    else:
      review_tensor = len(self.vocab)*torch.ones(self.max_len)
    for i,word in enumerate(review):
        review_tensor[i] = self.vocab.index(word)
    return review_tensor/len(self.vocab) # normalizing

  @staticmethod
  def _sentiment_to_tensor(sentiment):     
    sentiment_tensor = torch.zeros(2)
    if sentiment is 1:
        sentiment_tensor[1] = 1
    elif sentiment is 0: 
        sentiment_tensor[0] = 1
    sentiment_tensor = sentiment_tensor.type(torch.long)
    return sentiment_tensor

  @staticmethod
  def _corpus_stats(indexed_dataset):
    len_stat = []
    for i, data in enumerate(indexed_dataset):
      review, cat, label = data
      len_stat.append(len(review))
    min_len = min(len_stat)
    max_len = max(len_stat)
    return [min_len,max_len]

# model def
class GRUnet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size=2, n_layers=2, drop_prob=0.2):
        super().__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.gru = nn.GRU(input_size, hidden_size, n_layers, batch_first=True, dropout=drop_prob)
        self.fc = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        self.logsoftmax = nn.LogSoftmax(dim=1)
        
    def forward(self, x, h):
        out, h = self.gru(x, h)
        out = self.fc(self.relu(out[:,-1]))
        out = self.logsoftmax(out)
        return out, h

    def init_hidden(self, batch_size):
        weight = next(self.parameters()).data
        hidden = weight.new(self.n_layers, batch_size, self.hidden_size).zero_()
        return hidden

#training
def run_training_int(net, train_dataloader, device, params):
  lr = params['lr']
  momentum= params['momentum']
  num_epochs = params['num epochs']
  freq= params['freq']

  net.to(device)
  crit = nn.NLLLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)
  for epoch in range(num_epochs):
    running_loss =0.0
    for i, data in enumerate(train_dataloader):
      review_tensor, category, sentiment = data['review'], data['category'], data['sentiment']
      review_tensor, sentiment = review_tensor.to(device), sentiment.to(device) 
      optimizer.zero_grad()
      hidden = net.init_hidden(1).to(device)
      for iw in range(review_tensor.shape[1]):
        inp = review_tensor[0,iw].unsqueeze(0).unsqueeze(0).unsqueeze(0).to(device)
        output, hidden = net(inp,hidden)
      loss = crit(output,torch.argmax(sentiment,1))
      running_loss += loss.item()
      loss.backward()
      optimizer.step()
      if (i+1)%freq ==0:
        avg_loss = running_loss / float(freq)
        print('[epoch:%d iter: %d]\t loss:%0.4f'%(epoch+1,i+1, avg_loss))
        running_loss = 0.0

#testing
def run_testing_int(net, test_dataloader, device):
  net.to(device)
  negative_total = 0
  positive_total = 0
  confusion_matrix = torch.zeros(2,2)
  with torch.no_grad():
    for i, data in enumerate(test_dataloader):
      review_tensor, category, sentiment = data['review'], data['category'], data['sentiment']
      review_tensor, sentiment = review_tensor.to(device), sentiment.to(device) 
      hidden = net.init_hidden(1).to(device)
      for iw in range(review_tensor.shape[1]):
        inp = review_tensor[0,iw].unsqueeze(0).unsqueeze(0).unsqueeze(0).to(device)
        output, hidden = net(inp,hidden)
      predicted_sentiment = torch.argmax(output,1)
      gt_sentiment = torch.argmax(sentiment,1)
      for b in range(gt_sentiment.shape[0]): 
        confusion_matrix[gt_sentiment[b], predicted_sentiment[b]] += 1
        if gt_sentiment[b]== 0:
          negative_total += 1
        elif gt_sentiment[b]== 1:
          positive_total += 1
  #print confusion matrix  
  out_percent = np.zeros((2,2), dtype='float')
  out_str = "                      "
  out_str +=  "%18s    %18s" % ('predicted negative', 'predicted positive')
  print(out_str + "\n")
  for i,label in enumerate(['true negative', 'true positive']):
    out_percent[0,0] = "%.3f" % (100 * confusion_matrix[0,0] / float(negative_total))
    out_percent[0,1] = "%.3f" % (100 * confusion_matrix[0,1] / float(negative_total))
    out_percent[1,0] = "%.3f" % (100 * confusion_matrix[1,0] / float(positive_total))
    out_percent[1,1] = "%.3f" % (100 * confusion_matrix[1,1] / float(positive_total))
    out_str = "%12s:  " % label
    for j in range(2):
        out_str +=  "%18s" % out_percent[i,j]
    print(out_str)

print('+Task1:')
dataset_size='200'
batch_size=1
hidden_size=512
train_params = {'lr':1e-4,
                'momentum': 0.9,
                'num epochs': 1,
                'freq':100}
#datasets
train_dataset = Word_as_Int_Dataset(dataset_size)
test_dataset = Word_as_Int_Dataset(dataset_size,train=False)
#dataloaders
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
test_dataloader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)
#model
model= GRUnet(1, hidden_size)
#training
run_training_int(model, train_dataloader, device, train_params)
#testing
run_testing_int(model, test_dataloader, device)

"""###(1.2) Using word2vec representations (Not used)

#### Learn word embeddings
"""

#dataset def: not used 
class Whole_corpus_Dataset(Dataset):
  root = './gdrive/My Drive/ECE695_DL/HW6/data/'
  def __init__(self, dataset_size, window_size=5, train=True):
    super().__init__()    
    self.train= train
    self.dataset_size = dataset_size
    self.window_size = window_size
    if train:
      dataset_name = self.root+'sentiment_dataset_train_'+dataset_size+'.tar.gz'
    else:
      dataset_name = self.root+'sentiment_dataset_test_'+dataset_size+'.tar.gz'
    f = gzip.open(dataset_name, 'rb')
    dataset = f.read()
    if sys.version_info[0] == 3:
      self.positive_reviews, self.negative_reviews, self.vocab = pickle.loads(dataset, encoding='latin1')
    else:
      self.positive_reviews, self.negative_reviews, self.vocab = pickle.loads(dataset)
    self.vocab = sorted(self.vocab)
    self.vocab_size = len(self.vocab)
    self.categories = sorted(list(self.positive_reviews.keys()))
    self.category_sizes_pos = {category : len(self.positive_reviews[category]) for category in self.categories}
    self.category_sizes_neg = {category : len(self.negative_reviews[category]) for category in self.categories}
    self.indexed_dataset = []

    for category in self.positive_reviews:
        for review in self.positive_reviews[category]:
            self.indexed_dataset.append([review, category, 1])
    for category in self.negative_reviews:
        for review in self.negative_reviews[category]:
            self.indexed_dataset.append([review, category, 0])
    random.shuffle(self.indexed_dataset)

    self.cumsum_mask_lens, self.data_mask = self._corpus_stats(self.indexed_dataset)

  def __len__(self): 
    return self.cumsum_mask_lens[-1]

  def __getitem__(self, global_idx):
    list_idx = np.nonzero(global_idx<self.cumsum_mask_lens)[0][0]
    cumsum = list(self.cumsum_mask_lens)
    cumsum.reverse()
    cumsum.append(0)
    cumsum.reverse()
    idx = global_idx - cumsum[list_idx]
    global_list_idx = self.data_mask(list_idx)
    sample = self.indexed_dataset[global_list_idx] 
    review = sample[0]
    window_words = [review[i+idx] for i in range(self.window_size)]
    focus_word_tensor, ctx_words_tensor = self._encode(window_words, self.vocab, self.window_size)
    sample ={'focus_word': focus_word_tensor,
             'ctx_tensor': ctx_words_tensor}
    return sample

    @staticmethod
    def _corpus_stats(indexed_dataset, window_size):
      len_stat = []
      data_mask= [] # mask with useable indices
      for i, data in enumerate(indexed_dataset):
        review, cat, label = data
        len_stat.append(len(review))
        if len(review) >= window_size: data_mask.append(i) # too short review
      cumsum_mask_lens = np.cumsum(np.array(len_stat)[data_mask] - window_size + 1) 
      return cumsum_mask_lens, np.array(data_mask)  
    
    @staticmethod
    def _encode(window_words, vocab, window_size):
      focus_word_tensor = torch.zeros(len(self.vocab))
      focus_word_tensor[self.vocab.index(window_words[window_size//2])] = 1
      ctx_words_tensor = torch.zeros(len(self.vocab))
      for i in range(window_size):
        ictx_idx = self.vocab.index(window_words[i])
        ctx_words_tensor[ictx_idx] = 1/window_size
      return focus_word_tensor, ctx_words_tensor

#model def for word2vec: not used
class SkipGramNet(nn.Module):
  def __ini__(self,vocab_size, mid_size=512):
    self.mid_size= mid_size
    self.vocab_size = vocab_size
    self.linear1 = nn.Linear(self.vocab_size, self.mid_size, bias=False)
    self.linear2 = nn.Linear(self.mid_size, self.vocab_size, bias=False)

  def forward(self, x):
    mid = self.linear1(x)
    out = self.linear2(mid)
    return out, mid

# training :not used
def run_training_w2v(net, train_dataloader, device, params):
  lr = params['lr']
  momentum= params['momentum']
  num_epochs = params['num epochs']
  freq= params['freq']
  net.to(device)
  crit = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)
  for epoch in range(num_epochs):
    running_loss =0.0
    for i, data in enumerate(train_dataloader):
      x, gt_label = data['focus_word'], data['ctx_tensor']
      x, gt_label = x.to(device), gt_label.to(device) 
      optimizer.zero_grad()
      output, mid = net(x)
      loss = crit(output, gt_label)
      running_loss += loss.item()
      loss.backward()
      optimizer.step()
      if (i+1)%freq ==0:
        avg_loss = running_loss / float(freq)
        print('[epoch:%d iter: %d]\t loss:%0.4f'%(epoch+1, i+1, avg_loss))
        running_loss = 0.0
# testing: not implemented

"""#### Use word embeddings to represent the vocab (not implemented)"""

"""
#(2) Task-2: Incorporating gating mechanism in `TEXTnetOrder2`
"""

# Modified TEXTnetOrder2
class TEXTnetOrder2_mod(nn.Module):
  """manually writing LSTM cell with peepholes introduced by Gers & Schmidhuber (2000)"""
  def __init__(self, input_size, hidden_size, output_size, device):
    super().__init__()
    self.input_size= input_size
    self.hidden_size= hidden_size # same as cell state size
    self.cell_size = hidden_size
    self.output_size = output_size
    self.linear_forget_gate = nn.Linear(self.input_size+ 2*self.hidden_size, self.cell_size)
    self.linear_info_gate = nn.Linear(self.input_size+ 2*self.hidden_size, self.cell_size)
    self.linear_new_info = nn.Linear(self.input_size+ self.hidden_size, self.cell_size)
    self.linear_output_gate = nn.Linear(self.input_size+ 2*self.hidden_size, self.cell_size)
    self.linear_hidden2mid = nn.Linear(self.hidden_size, 100)
    self.linear_mid2out = nn.Linear(100, self.output_size)
    self.logsoftmax = nn.LogSoftmax(dim=1)

  def forward(self, x,h, cell):
    xh = torch.cat((x,h),dim=1)
    xhc = torch.cat((x,h,cell),dim=1)
    info_gate = F.sigmoid(self.linear_info_gate(xhc))
    forget_gate = F.sigmoid(self.linear_forget_gate(xhc))
    transformed_new_info = F.tanh(self.linear_new_info(xh))
    output_gate = F.sigmoid(self.linear_output_gate(xhc))
    cell = forget_gate*cell + info_gate*transformed_new_info
    hidden = output_gate*F.tanh(cell)
    out = F.relu(self.linear_hidden2mid(hidden))
    out = self.linear_mid2out(out)
    out = self.logsoftmax(out)
    return out, hidden, cell
  
  def init_hidden(self, batch_size):
    return torch.zeros(batch_size, self.hidden_size)

#training
def run_training_task2_int(net, train_dataloader, device, params):
  lr = params['lr']
  momentum= params['momentum']
  num_epochs = params['num epochs']
  freq= params['freq']

  net.to(device)
  crit = nn.NLLLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)
  for epoch in range(num_epochs):
    running_loss =0.0
    for i, data in enumerate(train_dataloader):
      review_tensor, category, sentiment = data['review'], data['category'], data['sentiment']
      review_tensor, sentiment = review_tensor.to(device), sentiment.to(device) 
      optimizer.zero_grad()
      hidden = net.init_hidden(review_tensor.shape[0]).to(device)
      cell= net.init_hidden(review_tensor.shape[0]).to(device)
      for iw in range(review_tensor.shape[1]):
        inp = review_tensor[0,iw].unsqueeze(0).unsqueeze(0).to(device)
        output, hidden, cell = net(inp,hidden,cell)
      loss = crit(output,torch.argmax(sentiment,1))
      running_loss += loss.item()
      loss.backward()
      optimizer.step()
      if (i+1)%freq ==0:
        avg_loss = running_loss / float(freq)
        print('[epoch:%d iter: %d]\t loss:%0.4f'%(epoch+1,i+1, avg_loss))
        running_loss = 0.0

#testing
def run_testing_task2_int(net, test_dataloader, device):
  net.to(device)
  negative_total = 0
  positive_total = 0
  confusion_matrix = torch.zeros(2,2)
  with torch.no_grad():
    for i, data in enumerate(test_dataloader):
      review_tensor, category, sentiment = data['review'], data['category'], data['sentiment']
      review_tensor, sentiment = review_tensor.to(device), sentiment.to(device) 
      hidden = net.init_hidden(review_tensor.shape[0]).to(device)
      cell = net.init_hidden(review_tensor.shape[0]).to(device)
      for iw in range(review_tensor.shape[1]):
        inp = review_tensor[0,iw].unsqueeze(0).unsqueeze(0).to(device)
        output, hidden,cell  = net(inp,hidden,cell)
      predicted_sentiment = torch.argmax(output,1)
      gt_sentiment = torch.argmax(sentiment,1)
      for b in range(gt_sentiment.shape[0]): 
        confusion_matrix[gt_sentiment[b], predicted_sentiment[b]] += 1
        if gt_sentiment[b]== 0:
          negative_total += 1
        elif gt_sentiment[b]== 1:
          positive_total += 1
  #print confusion matrix  
  out_percent = np.zeros((2,2), dtype='float')
  out_str = "                      "
  out_str +=  "%18s    %18s" % ('predicted negative', 'predicted positive')
  print(out_str + "\n")
  for i,label in enumerate(['true negative', 'true positive']):
    out_percent[0,0] = "%.3f" % (100 * confusion_matrix[0,0] / float(negative_total))
    out_percent[0,1] = "%.3f" % (100 * confusion_matrix[0,1] / float(negative_total))
    out_percent[1,0] = "%.3f" % (100 * confusion_matrix[1,0] / float(positive_total))
    out_percent[1,1] = "%.3f" % (100 * confusion_matrix[1,1] / float(positive_total))
    out_str = "%12s:  " % label
    for j in range(2):
        out_str +=  "%18s" % out_percent[i,j]
    print(out_str)

print('+Task2:')
dataset_size='200'
batch_size=1
hidden_size=512
train_params = {'lr':1e-4,
                'momentum': 0.9,
                'num epochs': 1,
                'freq':100}
#datasets
train_dataset = Word_as_Int_Dataset(dataset_size)
test_dataset = Word_as_Int_Dataset(dataset_size,train=False)
#dataloaders
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
test_dataloader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)
#model
model= TEXTnetOrder2_mod(1, hidden_size, 2, device=device)
#training
run_training_task2_int(model, train_dataloader, device, train_params)
#testing
run_testing_task2_int(model, test_dataloader, device)

"""#(3) Task-3: Batch Learning using:
Padding each review with integer index corresponding to new empty character added to vocab so that all samples have length as that of the longest review.
"""

#training
def run_training_task3_int(net, train_dataloader, device, params):
  lr = params['lr']
  momentum= params['momentum']
  num_epochs = params['num epochs']
  freq= params['freq']

  net.to(device)
  crit = nn.NLLLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)
  for epoch in range(num_epochs):
    running_loss =0.0
    for i, data in enumerate(train_dataloader):
      review_tensor, category, sentiment = data['review'], data['category'], data['sentiment']
      review_tensor, sentiment = review_tensor.to(device), sentiment.to(device) 
      optimizer.zero_grad()
      hidden = net.init_hidden(review_tensor.shape[0]).to(device)
      cell = net.init_hidden(review_tensor.shape[0]).to(device)
      for iw in range(review_tensor.shape[1]):
        inp = review_tensor[:,iw].unsqueeze(1).to(device)
        output, hidden, cell = net(inp,hidden,cell )          
      loss = crit(output,torch.argmax(sentiment,1))
      running_loss += loss.item()
      loss.backward()
      optimizer.step()
      if (i+1)%freq ==0:
        avg_loss = running_loss / float(freq)
        print('[epoch:%d iter: %d]\t loss:%0.4f'%(epoch+1,i+1, avg_loss))
        running_loss = 0.0

#testing
def run_testing_task3_int(net, test_dataloader, device):
  net.to(device)
  negative_total = 0
  positive_total = 0
  confusion_matrix = torch.zeros(2,2)
  with torch.no_grad():
    for i, data in enumerate(test_dataloader):
      review_tensor, category, sentiment = data['review'], data['category'], data['sentiment']
      review_tensor, sentiment = review_tensor.to(device), sentiment.to(device) 
      hidden = net.init_hidden(review_tensor.shape[0]).to(device)
      cell = net.init_hidden(review_tensor.shape[0]).to(device)
      for iw in range(review_tensor.shape[1]):
        inp = review_tensor[:,iw].unsqueeze(1).to(device)
        output, hidden,cell = net(inp,hidden,cell)
      predicted_sentiment = torch.argmax(output,1)
      gt_sentiment = torch.argmax(sentiment,1)
      for b in range(gt_sentiment.shape[0]): 
        confusion_matrix[gt_sentiment[b], predicted_sentiment[b]] += 1
        if gt_sentiment[b]== 0:
          negative_total += 1
        elif gt_sentiment[b]== 1:
          positive_total += 1
  #print confusion matrix  
  out_percent = np.zeros((2,2), dtype='float')
  out_str = "                      "
  out_str +=  "%18s    %18s" % ('predicted negative', 'predicted positive')
  print(out_str + "\n")
  for i,label in enumerate(['true negative', 'true positive']):
    out_percent[0,0] = "%.3f" % (100 * confusion_matrix[0,0] / float(negative_total))
    out_percent[0,1] = "%.3f" % (100 * confusion_matrix[0,1] / float(negative_total))
    out_percent[1,0] = "%.3f" % (100 * confusion_matrix[1,0] / float(positive_total))
    out_percent[1,1] = "%.3f" % (100 * confusion_matrix[1,1] / float(positive_total))
    out_str = "%12s:  " % label
    for j in range(2):
        out_str +=  "%18s" % out_percent[i,j]
    print(out_str)

print('+Task3:')
dataset_size='200'
batch_size=8
hidden_size=512
train_params = {'lr':1e-4,
                'momentum': 0.9,
                'num epochs': 1,
                'freq':100}
#datasets
train_dataset = Word_as_Int_Dataset(dataset_size,pad=True)
test_dataset = Word_as_Int_Dataset(dataset_size,train=False,pad=True)
#dataloaders
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
test_dataloader =  DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)
#model
model= TEXTnetOrder2_mod(1, hidden_size, 2, device=device)
#training
run_training_task3_int(model, train_dataloader, device, train_params)
#testing
run_testing_task3_int(model, test_dataloader, device)
