# -*- coding: utf-8 -*-
"""hw04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1umvRRW_FH_19OhPV-dBpXPQhzjfmUcv-

ResNet-type Skip connection blocks\
author: rahul deshmukh\
email: deshmuk5@purdue.edu

Sources:
1. https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\
2. https://arxiv.org/pdf/1512.03385.pdf
3. http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006
"""

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchsummary import summary

from google.colab import drive
drive.mount('/content/gdrive')

"""## (1) Network definition"""

class SkipBlock(nn.Module):
  def __init__(self, in_ch, out_ch, downsample=False, skip_connections=True):
      super().__init__()
      self.downsample = downsample
      self.skip_connections = skip_connections
      self.in_ch = in_ch
      self.out_ch = out_ch
      self.convo = nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1)
      norm_layer = nn.BatchNorm2d
      self.bn = norm_layer(out_ch)
      if downsample:
          self.downsampler = nn.Conv2d(in_ch, out_ch, 1, stride=2)

  def forward(self, x):
      identity = x                                     
      out = self.convo(x)                              
      out = self.bn(out)                              
      out = F.relu(out)
      if self.in_ch == self.out_ch:
          out = self.convo(out)                              
          out = self.bn(out)                              
          out = F.relu(out)
      if self.downsample:
          out = self.downsampler(out)
          identity = self.downsampler(identity)
      if self.skip_connections:
          if self.in_ch == self.out_ch:
              out += identity                              
          else:
              out[:,:self.in_ch,:,:] += identity
              out[:,self.in_ch:,:,:] += identity
      return out

class BMEnet(nn.Module):
    def __init__(self, skip_connections=True, depth=32):
        super().__init__()
        self.pool_count = 3
        self.depth = depth // 2
        self.conv = nn.Conv2d(3, 64, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.skip64 = SkipBlock(64, 64, skip_connections=skip_connections)
        self.skip64ds = SkipBlock(64, 64, downsample=True, skip_connections=skip_connections)
        self.skip64to128 = SkipBlock(64, 128,skip_connections=skip_connections )
        self.skip128 = SkipBlock(128, 128, skip_connections=skip_connections)
        self.skip128ds = SkipBlock(128,128,downsample=True, skip_connections=skip_connections)
        self.fc1 =  nn.Linear(128 * (32 // 2**self.pool_count)**2, 1000)
        self.fc2 =  nn.Linear(1000, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv(x)))          
        for _ in range(self.depth // 4):
            x = self.skip64(x)                                               
        x = self.skip64ds(x)
        for _ in range(self.depth // 4):
            x = self.skip64(x)                                               
        x = self.skip64to128(x)
        for _ in range(self.depth // 4):
            x = self.skip128(x)                                               
        x = self.skip128ds(x)                                               
        for _ in range(self.depth // 4):
            x = self.skip128(x)                                               
        x = x.view(-1, 128 * (32 // 2**self.pool_count)**2 )
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class BottleNeck_Block(nn.Module):
  """ 3-conv layer bottleneck block"""
  def __init__(self,in_channels,mid_channels,out_channels, downsample=False, skip_connections=True):
    super().__init__()
    self.in_channels = in_channels
    self.mid_channels = mid_channels
    self.out_channels = out_channels
    self.downsample = downsample
    self.skip_connections = skip_connections

    norm_layer = nn.BatchNorm2d
    self.bn_in = norm_layer(in_channels)
    self.bn_mid = norm_layer(mid_channels)
    self.bn_out = norm_layer(out_channels)    
    if downsample:
      self.conv1 = nn.Conv2d(in_channels, mid_channels, 1, stride=2)
      self.downsampler = nn.Conv2d(in_channels,in_channels, 1, stride=2)
    else:
      self.conv1 = nn.Conv2d(in_channels, mid_channels, 1)
    self.conv2 = nn.Conv2d(mid_channels,mid_channels,3,padding=1)
    self.conv3 = nn.Conv2d(mid_channels,out_channels, 1)    

  def forward(self,x):
    identity = x.clone()
    # first conv layer
    out = self.conv1(x)
    out = self.bn_mid(out)
    out = F.relu(out)
    # second conv layer
    out = self.conv2(out)
    out = self.bn_mid(out)
    out = F.relu(out)
    # third conv layer
    out = self.conv3(out)
    out = self.bn_out(out)
    out = F.relu(out)
    if self.downsample:
        identity = self.downsampler(identity)
        identity = self.bn_in(identity)
    if self.skip_connections:
      if self.in_channels == self.out_channels:
          out += identity                              
      else:
        for num in range(self.out_channels//self.in_channels):
          out[:,num*self.in_channels:(num+1)*self.in_channels,:,:] += identity
    return(F.relu(out))

class ResNet_BottleNeck(nn.Module):
  def __init__(self,config):
    super().__init__()
    self.config = config
    self.conv = nn.Conv2d(3,64,3,padding=1) # same conv 
    self.pool = nn.MaxPool2d(2,2)

    
    # self.btlnck_blk_64_256_ds = BottleNeck_Block(64,64,256,downsample=True)
    self.btlnck_blk_64_256 = BottleNeck_Block(64,64,256)
    self.btlnck_blk256 = BottleNeck_Block(256,64,256)

    self.btlnck_blk_256_512_ds = BottleNeck_Block(256,128,512,downsample=True)
    self.btlnck_blk_256_512 = BottleNeck_Block(256,128,512)
    self.btlnck_blk512 = BottleNeck_Block(512,128,512)

    self.btlnck_blk_512_1024_ds = BottleNeck_Block(512,256,1024,downsample=True)
    self.btlnck_blk_512_1024 = BottleNeck_Block(512,256,1024)
    self.btlnck_blk1024 = BottleNeck_Block(1024,256,1024)

    self.av_pool = nn.AvgPool2d(4,1)
    self.fc1 =  nn.Linear(1024, 10)

  def forward(self,x):
    x = self.pool(F.relu(self.conv(x)))
    x = self.btlnck_blk_64_256(x)
    for _ in range(self.config[0]):
      x = self.btlnck_blk256(x)                                               
    x = self.btlnck_blk_256_512_ds(x)
    for _ in range(self.config[1]):
      x = self.btlnck_blk512(x)                                               
    x = self.btlnck_blk_512_1024_ds(x)
    for _ in range(self.config[2]):
      x = self.btlnck_blk1024(x)
    x = self.av_pool(x)
    x = x.view(-1, 1024)
    x = F.relu(self.fc1(x))
    return x 

class ResNet50(ResNet_BottleNeck):
  config = np.array([3,4,6,3]) - 1
  def __init__(self, config=config):
    super().__init__(config)

class ResNet101(ResNet_BottleNeck):
  config = np.array([3,4,23,3]) - 1
  def __init__(self, config=config):
    super().__init__(config)

class ResNet152(ResNet_BottleNeck):
  config = np.array([3,8,36,3]) - 1
  def __init__(self, config=config):
    super().__init__(config)



def run_code_for_training(train_dataloader, epochs, device):
  net.to(device)
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, momentum=0.9)
  train_loss=[]  
  epoch_loss=[]
  for epoch in range(epochs):  
      print("\n")
      running_loss = 0.0
      running_epoch_loss=0.0
      for i, data in enumerate(train_dataloader):
          inputs, labels = data
          inputs = inputs.to(device)
          labels = labels.to(device)
          optimizer.zero_grad()
          # Make the predictions with the model:
          outputs = net(inputs)
          loss = criterion(outputs, labels)

          loss.backward()
          optimizer.step()
          running_loss += loss.item()
          running_epoch_loss +=loss.item()
          if (i+1)%1250 == 0:    
              avg_loss = running_loss / float(1250)
              train_loss.append(avg_loss)
              print("[epoch:%d, batch:%5d] batch avergaged loss: %.3f" % (epoch + 1, i + 1, avg_loss))
              running_loss = 0.0
      epoch_loss.append(running_epoch_loss)
      running_epoch_loss=0.0
  print("\nFinished Training\n")
  return(epoch_loss)

def run_code_for_testing(net, test_data_loader, device):
  accuracy=0
  count=0
  with torch.set_grad_enabled(False):
    net.eval()
    net.to(device)  
    for i,data in enumerate(test_data_loader):
      x, y_true = data
      x, y_true = x.to(device), y_true.to(device)
      y_pred = net(x)
      _, y_pred = torch.max(F.softmax(y_pred, dim=1), dim=1)
      for b in range(y_pred.shape[0]): 
        if y_true[b]==y_pred[b]: accuracy+=1
      count += y_pred.shape[0]
    accuracy = accuracy/count
  return accuracy*100

"""## (2) Import CIFAR-10 data"""

batch_size = 8

transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
# define datasets
train_dataset  = torchvision.datasets.CIFAR10(root='./train', train=True,
                                              download=True, transform=transform)
test_dataset  = torchvision.datasets.CIFAR10(root='./test', train=False,
                                              download=True, transform=transform)
# define dataloader
train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size,
                                                shuffle=True, num_workers=4)
test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=batch_size,
                                              shuffle=False, num_workers=4)

"""##(3) Print network summary"""

#net = BMEnet()
net = ResNet_50()
# summary(net, (3,32,32),-1,device='cpu')

"""##(3) Learn using CIFAR-10 data"""

# Commented out IPython magic to ensure Python compatibility.
if torch.cuda.is_available():
  device = torch.device("cuda:0")
else:
  device = torch.device("cpu")

epochs=10

train_loss= run_code_for_training(train_dataloader, epochs, device)
torch.save(net.state_dict(),'./wts.pkl')
# %cp ./wts.pkl ./gdrive/My\ Drive/ECE695_DL/HW4/
classification_accuracy= run_code_for_testing(net,test_dataloader,device)

plt.plot(np.arange(1,len(train_loss)+1,1)*1250,train_loss)
plt.xlabel('iter')
plt.ylabel('training loss')

# Commented out IPython magic to ensure Python compatibility.
# print to file
with open('./output.txt','w') as f:
  for i,loss in enumerate(train_loss):
    f.write('Epoch  %d:\t %0.4f\n'%(i, train_loss[i]))
  f.write('\n')
  f.write('Test Accuracy:\t %0.4f%%'%(classification_accuracy))

f.close()
# %cp ./output.txt ./gdrive/My\ Drive/ECE695_DL/HW4/

# Commented out IPython magic to ensure Python compatibility.
# %cat output.txt