{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw04.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlPQatUMAZhN",
        "colab_type": "text"
      },
      "source": [
        "ResNet-type Skip connection blocks\\\n",
        "author: rahul deshmukh\\\n",
        "email: deshmuk5@purdue.edu\n",
        "\n",
        "Sources:\n",
        "1. https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\\\n",
        "2. https://arxiv.org/pdf/1512.03385.pdf\n",
        "3. http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
        "4. https://engineering.purdue.edu/kak/distDLS/DLStudio-1.0.4.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgSIT4ssA-YU",
        "colab_type": "code",
        "outputId": "d618e959-62af-4e5f-8125-b9fde3068aa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import csv,os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage import io, transform\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchsummary import summary\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-D6aDM9BXt_",
        "colab_type": "text"
      },
      "source": [
        "## (1) Network definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP9ky7dcBMGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BottleNeck_Block(nn.Module):\n",
        "  \"\"\" 3-conv layer bottleneck block\"\"\"\n",
        "  def __init__(self,in_channels,mid_channels,out_channels, downsample=False, skip_connections=True):\n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.mid_channels = mid_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.downsample = downsample\n",
        "    self.skip_connections = skip_connections\n",
        "\n",
        "    norm_layer = nn.BatchNorm2d\n",
        "    self.bn_in = norm_layer(in_channels)\n",
        "    self.bn_mid = norm_layer(mid_channels)\n",
        "    self.bn_out = norm_layer(out_channels)    \n",
        "    if downsample:\n",
        "      self.conv1 = nn.Conv2d(in_channels, mid_channels, 1, stride=2)\n",
        "      self.downsampler = nn.Conv2d(in_channels,in_channels, 1, stride=2)\n",
        "    else:\n",
        "      self.conv1 = nn.Conv2d(in_channels, mid_channels, 1)\n",
        "      # self.downsampler = nn.Conv2d(in_channels,out_channels, 1)\n",
        "    self.conv2 = nn.Conv2d(mid_channels,mid_channels,3,padding=1)\n",
        "    self.conv3 = nn.Conv2d(mid_channels,out_channels, 1)    \n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    identity = x.clone()\n",
        "    # first conv layer\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn_mid(out)\n",
        "    out = F.relu(out)\n",
        "    # second conv layer\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn_mid(out)\n",
        "    out = F.relu(out)\n",
        "    # third conv layer\n",
        "    out = self.conv3(out)\n",
        "    out = self.bn_out(out)\n",
        "    if self.downsample:\n",
        "      identity = self.downsampler(identity)\n",
        "      identity = self.bn_in(identity)\n",
        "    # elif self.in_channels != self.out_channels: \n",
        "    #   identity = self.downsampler(identity)\n",
        "    #   identity = self.bn_out(identity)\n",
        "      \n",
        "    if self.skip_connections:\n",
        "      if self.in_channels == self.out_channels:\n",
        "          out += identity                              \n",
        "      else:\n",
        "        for num in range(self.out_channels//self.in_channels):\n",
        "          out[:,num*self.in_channels:(num+1)*self.in_channels,:,:] += identity\n",
        "        # out += identity\n",
        "    return(F.relu(out))\n",
        "\n",
        "class ResNet_BottleNeck(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.conv = nn.Conv2d(3,64,7,stride=2,padding=3) \n",
        "    self.pool = nn.MaxPool2d(3,stride=2, padding=1)\n",
        "    \n",
        "    # self.btlnck_blk_64_256_ds = BottleNeck_Block(64,64,256,downsample=True)\n",
        "    self.btlnck_blk_64_256 = BottleNeck_Block(64,64,256)\n",
        "    self.btlnck_blk256 = BottleNeck_Block(256,64,256)\n",
        "\n",
        "    self.btlnck_blk_256_512_ds = BottleNeck_Block(256,128,512,downsample=True)\n",
        "    self.btlnck_blk512 = BottleNeck_Block(512,128,512)\n",
        "\n",
        "    self.btlnck_blk_512_1024_ds = BottleNeck_Block(512,256,1024,downsample=True)\n",
        "    self.btlnck_blk1024 = BottleNeck_Block(1024,256,1024)\n",
        "\n",
        "    self.btlnck_blk_1024_2048_ds = BottleNeck_Block(1024,512,2048,downsample=True)\n",
        "    self.btlnck_blk2048 = BottleNeck_Block(2048,512,2048)\n",
        "\n",
        "    self.av_pool = nn.AvgPool2d(7,1)\n",
        "    self.fc1 =  nn.Linear(2048, 5)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(F.relu(self.conv(x)))\n",
        "    x = self.btlnck_blk_64_256(x)\n",
        "    for _ in range(self.config[0]-1):\n",
        "      x = self.btlnck_blk256(x)                                               \n",
        "    x = self.btlnck_blk_256_512_ds(x)\n",
        "    for _ in range(self.config[1]-1):\n",
        "      x = self.btlnck_blk512(x)                                               \n",
        "    x = self.btlnck_blk_512_1024_ds(x)\n",
        "    for _ in range(self.config[2]-1):\n",
        "      x = self.btlnck_blk1024(x)\n",
        "    x = self.btlnck_blk_1024_2048_ds(x)\n",
        "    for _ in range(self.config[3]-1):\n",
        "      x = self.btlnck_blk2048(x)\n",
        "    x = self.av_pool(x)\n",
        "    x = x.view(-1, 2048)\n",
        "    x = self.fc1(x)\n",
        "    return x \n",
        "\n",
        "class ResNet50(ResNet_BottleNeck):\n",
        "  config = np.array([3,4,6,3]) \n",
        "  def __init__(self, config=config):\n",
        "    super().__init__(config)\n",
        "\n",
        "class ResNet101(ResNet_BottleNeck):\n",
        "  config = np.array([3,4,23,3]) \n",
        "  def __init__(self, config=config):\n",
        "    super().__init__(config)\n",
        "\n",
        "class ResNet152(ResNet_BottleNeck):\n",
        "  config = np.array([3,8,36,3]) \n",
        "  def __init__(self, config=config):\n",
        "    super().__init__(config)\n",
        "\n",
        "def run_code_for_training(net,train_dataloader, epochs, device, datalen):\n",
        "  net.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, momentum=0.9)\n",
        "  optimizer = torch.optim.Adam(net.parameters(),lr=1e-4)\n",
        "  epoch_loss=[]\n",
        "  for epoch in range(epochs):  \n",
        "      print(\"\\n\")\n",
        "      running_loss = 0.0\n",
        "      running_epoch_loss=0.0\n",
        "      for i, data in enumerate(train_dataloader):\n",
        "          inputs, labels = data\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          # Make the predictions with the model:\n",
        "          outputs = net(inputs)\n",
        "\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "          running_epoch_loss +=loss.item()\n",
        "          if (i+1)%200 == 0:    \n",
        "              avg_loss = running_loss / float(200)\n",
        "              print(\"[epoch:%d, batch:%5d] batch avergaged loss: %.3f\" % (epoch + 1, i + 1, avg_loss))\n",
        "              running_loss = 0.0\n",
        "      epoch_loss.append(running_epoch_loss/datalen)\n",
        "      running_epoch_loss=0.0\n",
        "  print(\"\\nFinished Training\\n\")\n",
        "  return(epoch_loss)\n",
        "\n",
        "def run_code_for_testing(net, test_data_loader, device):\n",
        "  accuracy=0\n",
        "  count=0\n",
        "  with torch.set_grad_enabled(False):\n",
        "    net.eval()\n",
        "    net.to(device)  \n",
        "    for i,data in enumerate(test_data_loader):\n",
        "      x, y_true = data\n",
        "      x, y_true = x.to(device), y_true.to(device)\n",
        "      y_pred = net(x)\n",
        "      _, y_pred = torch.max(F.softmax(y_pred, dim=1), dim=1)\n",
        "      for b in range(y_pred.shape[0]): \n",
        "        if y_true[b]==y_pred[b]: accuracy+=1\n",
        "      count += y_pred.shape[0]\n",
        "    accuracy = accuracy/count\n",
        "  return accuracy*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95hymHyoqiPL",
        "colab_type": "text"
      },
      "source": [
        "##(2) ImageNet experiment\n",
        "### Foreword:\n",
        "1. The data was downloaded using https://github.com/mf1024/ImageNet-Datasets-Downloader\n",
        "2. imagenet5 folder contains 5 classes and a total of 5517 images\n",
        "3. The data was partitioned into training and testing with 80:20 ratio. Such that each class follows this ratio\n",
        "4. Samples for training and testing were drawn after shuffling the data in each class.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGr0dsdlr6Qz",
        "colab_type": "text"
      },
      "source": [
        "### Custom dataset for ImageNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bR6VL6KsBOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% ----------------- Class Definitions --------------------------------------#\n",
        "class ImageNetDataset(Dataset):\n",
        "  in_channels=3\n",
        "  image_size = 224\n",
        "  def __init__(self,root_dir, train=True, transform = None):\n",
        "    self.root_dir = root_dir\n",
        "    self.train = train\n",
        "    self.transform = transform \n",
        "    if train: \n",
        "        self.csvfile = root_dir + 'train_data.csv'\n",
        "    else: \n",
        "        self.csvfile = root_dir + 'test_data.csv'\n",
        "    with open(self.csvfile,'rt') as f:\n",
        "      reader = csv.reader(f,delimiter=',')\n",
        "      self.data_list = [row for row in reader]\n",
        "\n",
        "    self.normalize = torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "  def __len__(self):\n",
        "    self.datalen = len(self.data_list)\n",
        "    return(self.datalen)\n",
        "      \n",
        "  def __getitem__(self,idx):\n",
        "    img_name = self.root_dir + self.data_list[idx][0]\n",
        "    class_id =  int(self.data_list[idx][1])\n",
        "    image = io.imread(img_name)/255        \n",
        "    #resize image and mask\n",
        "    image = transform.resize(image,(self.image_size, self.image_size, self.in_channels),\n",
        "                              anti_aliasing=False)        \n",
        "    # reshape from (H,W,C) -> (C,H,W)            \n",
        "    image = image.transpose(2,0,1)\n",
        "    image_tensor = torch.from_numpy(image.astype(np.float32))    \n",
        "    image_tensor = self.normalize(image_tensor)\n",
        "    # augmentation\n",
        "    if self.transform is not None: image_tensor = self.transform(image_tensor)                 \n",
        "    return(image_tensor, class_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npAne08ZsjWP",
        "colab_type": "text"
      },
      "source": [
        "### Create ImageNet dataloaders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmm-0rcmsaRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = './gdrive/My Drive/ECE695_DL/HW4/imagenet5/'\n",
        "save_dir = './gdrive/My Drive/ECE695_DL/HW4/'\n",
        "batch_size = 4\n",
        "\n",
        "train_dataset = ImageNetDataset(root_dir)\n",
        "test_dataset = ImageNetDataset(root_dir, train=False)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrAiJcL2IQiI",
        "colab_type": "text"
      },
      "source": [
        "### Print Network summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKMVA9Q_IPgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = ResNet101()\n",
        "# summary(net, (3,224,224),-1,device='cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtXMqtquIbfB",
        "colab_type": "text"
      },
      "source": [
        "### Train network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZWW6oFRIbz2",
        "colab_type": "code",
        "outputId": "fc001a8e-4874-4cb8-ede2-1981b82c104e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "epochs=20\n",
        "\n",
        "if os.path.exists(save_dir+'wts50.pkl'):\n",
        "  net.load_state_dict(torch.load(save_dir+'wts50.pkl'))\n",
        "  epoch_counter = int(np.load(save_dir+'epoch_counter.npy'))\n",
        "else:\n",
        "  epoch_counter = 0\n",
        "\n",
        "train_loss= run_code_for_training(net, train_dataloader, epochs, device, train_dataset.datalen)\n",
        "classification_accuracy= run_code_for_testing(net,test_dataloader,device)\n",
        "# save wts and epochs\n",
        "torch.save(net.state_dict(), save_dir + 'wts50.pkl')\n",
        "np.save(save_dir+'epoch_counter.npy',epoch_counter+epochs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "[epoch:1, batch:  200] batch avergaged loss: 1.718\n",
            "[epoch:1, batch:  400] batch avergaged loss: 1.548\n",
            "[epoch:1, batch:  600] batch avergaged loss: 1.486\n",
            "[epoch:1, batch:  800] batch avergaged loss: 1.403\n",
            "[epoch:1, batch: 1000] batch avergaged loss: 1.430\n",
            "\n",
            "\n",
            "[epoch:2, batch:  200] batch avergaged loss: 1.358\n",
            "[epoch:2, batch:  400] batch avergaged loss: 1.363\n",
            "[epoch:2, batch:  600] batch avergaged loss: 1.282\n",
            "[epoch:2, batch:  800] batch avergaged loss: 1.293\n",
            "[epoch:2, batch: 1000] batch avergaged loss: 1.304\n",
            "\n",
            "\n",
            "[epoch:3, batch:  200] batch avergaged loss: 1.225\n",
            "[epoch:3, batch:  400] batch avergaged loss: 1.206\n",
            "[epoch:3, batch:  600] batch avergaged loss: 1.218\n",
            "[epoch:3, batch:  800] batch avergaged loss: 1.205\n",
            "[epoch:3, batch: 1000] batch avergaged loss: 1.173\n",
            "\n",
            "\n",
            "[epoch:4, batch:  200] batch avergaged loss: 1.121\n",
            "[epoch:4, batch:  400] batch avergaged loss: 1.150\n",
            "[epoch:4, batch:  600] batch avergaged loss: 1.167\n",
            "[epoch:4, batch:  800] batch avergaged loss: 1.106\n",
            "[epoch:4, batch: 1000] batch avergaged loss: 1.101\n",
            "\n",
            "\n",
            "[epoch:5, batch:  200] batch avergaged loss: 1.060\n",
            "[epoch:5, batch:  400] batch avergaged loss: 1.089\n",
            "[epoch:5, batch:  600] batch avergaged loss: 1.001\n",
            "[epoch:5, batch:  800] batch avergaged loss: 1.102\n",
            "[epoch:5, batch: 1000] batch avergaged loss: 1.060\n",
            "\n",
            "\n",
            "[epoch:6, batch:  200] batch avergaged loss: 0.981\n",
            "[epoch:6, batch:  400] batch avergaged loss: 1.051\n",
            "[epoch:6, batch:  600] batch avergaged loss: 1.065\n",
            "[epoch:6, batch:  800] batch avergaged loss: 0.983\n",
            "[epoch:6, batch: 1000] batch avergaged loss: 0.988\n",
            "\n",
            "\n",
            "[epoch:7, batch:  200] batch avergaged loss: 0.978\n",
            "[epoch:7, batch:  400] batch avergaged loss: 0.939\n",
            "[epoch:7, batch:  600] batch avergaged loss: 0.925\n",
            "[epoch:7, batch:  800] batch avergaged loss: 0.985\n",
            "[epoch:7, batch: 1000] batch avergaged loss: 0.918\n",
            "\n",
            "\n",
            "[epoch:8, batch:  200] batch avergaged loss: 0.921\n",
            "[epoch:8, batch:  400] batch avergaged loss: 0.964\n",
            "[epoch:8, batch:  600] batch avergaged loss: 0.880\n",
            "[epoch:8, batch:  800] batch avergaged loss: 0.855\n",
            "[epoch:8, batch: 1000] batch avergaged loss: 0.911\n",
            "\n",
            "\n",
            "[epoch:9, batch:  200] batch avergaged loss: 0.848\n",
            "[epoch:9, batch:  400] batch avergaged loss: 0.847\n",
            "[epoch:9, batch:  600] batch avergaged loss: 0.845\n",
            "[epoch:9, batch:  800] batch avergaged loss: 0.827\n",
            "[epoch:9, batch: 1000] batch avergaged loss: 0.850\n",
            "\n",
            "\n",
            "[epoch:10, batch:  200] batch avergaged loss: 0.738\n",
            "[epoch:10, batch:  400] batch avergaged loss: 0.735\n",
            "[epoch:10, batch:  600] batch avergaged loss: 0.811\n",
            "[epoch:10, batch:  800] batch avergaged loss: 0.830\n",
            "[epoch:10, batch: 1000] batch avergaged loss: 0.809\n",
            "\n",
            "\n",
            "[epoch:11, batch:  200] batch avergaged loss: 0.696\n",
            "[epoch:11, batch:  400] batch avergaged loss: 0.841\n",
            "[epoch:11, batch:  600] batch avergaged loss: 0.714\n",
            "[epoch:11, batch:  800] batch avergaged loss: 0.744\n",
            "[epoch:11, batch: 1000] batch avergaged loss: 0.770\n",
            "\n",
            "\n",
            "[epoch:12, batch:  200] batch avergaged loss: 0.717\n",
            "[epoch:12, batch:  400] batch avergaged loss: 0.681\n",
            "[epoch:12, batch:  600] batch avergaged loss: 0.701\n",
            "[epoch:12, batch:  800] batch avergaged loss: 0.675\n",
            "[epoch:12, batch: 1000] batch avergaged loss: 0.642\n",
            "\n",
            "\n",
            "[epoch:13, batch:  200] batch avergaged loss: 0.621\n",
            "[epoch:13, batch:  400] batch avergaged loss: 0.648\n",
            "[epoch:13, batch:  600] batch avergaged loss: 0.607\n",
            "[epoch:13, batch:  800] batch avergaged loss: 0.658\n",
            "[epoch:13, batch: 1000] batch avergaged loss: 0.616\n",
            "\n",
            "\n",
            "[epoch:14, batch:  200] batch avergaged loss: 0.568\n",
            "[epoch:14, batch:  400] batch avergaged loss: 0.555\n",
            "[epoch:14, batch:  600] batch avergaged loss: 0.558\n",
            "[epoch:14, batch:  800] batch avergaged loss: 0.587\n",
            "[epoch:14, batch: 1000] batch avergaged loss: 0.607\n",
            "\n",
            "\n",
            "[epoch:15, batch:  200] batch avergaged loss: 0.476\n",
            "[epoch:15, batch:  400] batch avergaged loss: 0.542\n",
            "[epoch:15, batch:  600] batch avergaged loss: 0.558\n",
            "[epoch:15, batch:  800] batch avergaged loss: 0.579\n",
            "[epoch:15, batch: 1000] batch avergaged loss: 0.574\n",
            "\n",
            "\n",
            "[epoch:16, batch:  200] batch avergaged loss: 0.470\n",
            "[epoch:16, batch:  400] batch avergaged loss: 0.515\n",
            "[epoch:16, batch:  600] batch avergaged loss: 0.460\n",
            "[epoch:16, batch:  800] batch avergaged loss: 0.543\n",
            "[epoch:16, batch: 1000] batch avergaged loss: 0.514\n",
            "\n",
            "\n",
            "[epoch:17, batch:  200] batch avergaged loss: 0.408\n",
            "[epoch:17, batch:  400] batch avergaged loss: 0.457\n",
            "[epoch:17, batch:  600] batch avergaged loss: 0.492\n",
            "[epoch:17, batch:  800] batch avergaged loss: 0.470\n",
            "[epoch:17, batch: 1000] batch avergaged loss: 0.505\n",
            "\n",
            "\n",
            "[epoch:18, batch:  200] batch avergaged loss: 0.342\n",
            "[epoch:18, batch:  400] batch avergaged loss: 0.355\n",
            "[epoch:18, batch:  600] batch avergaged loss: 0.405\n",
            "[epoch:18, batch:  800] batch avergaged loss: 0.412\n",
            "[epoch:18, batch: 1000] batch avergaged loss: 0.452\n",
            "\n",
            "\n",
            "[epoch:19, batch:  200] batch avergaged loss: 0.278\n",
            "[epoch:19, batch:  400] batch avergaged loss: 0.368\n",
            "[epoch:19, batch:  600] batch avergaged loss: 0.347\n",
            "[epoch:19, batch:  800] batch avergaged loss: 0.410\n",
            "[epoch:19, batch: 1000] batch avergaged loss: 0.431\n",
            "\n",
            "\n",
            "[epoch:20, batch:  200] batch avergaged loss: 0.287\n",
            "[epoch:20, batch:  400] batch avergaged loss: 0.334\n",
            "[epoch:20, batch:  600] batch avergaged loss: 0.293\n",
            "[epoch:20, batch:  800] batch avergaged loss: 0.335\n",
            "[epoch:20, batch: 1000] batch avergaged loss: 0.343\n",
            "\n",
            "Finished Training\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1N1QEhKIlYE",
        "colab_type": "text"
      },
      "source": [
        "### Print to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jwfvIEJIlqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(save_dir+'output.txt','a') as f:\n",
        "  for i,loss in enumerate(train_loss):\n",
        "    f.write('Epoch  %d:\\t %0.4f\\n'%(epoch_counter+i, train_loss[i]))\n",
        "f.close()\n",
        "with open(save_dir+'accuracy.txt','w') as f:\n",
        "  f.write('Test Accuracy:\\t %0.4f%%'%(classification_accuracy))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im_CxIBoz3hH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "d67e6756-6d47-4a9a-ff72-c869c7a334a1"
      },
      "source": [
        "%cat ./gdrive/My\\ Drive/ECE695_DL/HW4/output.txt\n",
        "%cat ./gdrive/My\\ Drive/ECE695_DL/HW4/accuracy.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0:\t 0.3759\n",
            "Epoch  1:\t 0.3287\n",
            "Epoch  2:\t 0.2992\n",
            "Epoch  3:\t 0.2807\n",
            "Epoch  4:\t 0.2653\n",
            "Epoch  5:\t 0.2523\n",
            "Epoch  6:\t 0.2362\n",
            "Epoch  7:\t 0.2255\n",
            "Epoch  8:\t 0.2115\n",
            "Epoch  9:\t 0.1969\n",
            "Epoch  10:\t 0.1901\n",
            "Epoch  11:\t 0.1723\n",
            "Epoch  12:\t 0.1591\n",
            "Epoch  13:\t 0.1437\n",
            "Epoch  14:\t 0.1373\n",
            "Epoch  15:\t 0.1260\n",
            "Epoch  16:\t 0.1158\n",
            "Epoch  17:\t 0.0990\n",
            "Epoch  18:\t 0.0910\n",
            "Epoch  19:\t 0.0814\n",
            "Test Accuracy:\t 18.0018%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLT28vY75V6L",
        "colab_type": "text"
      },
      "source": [
        "### Loss plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX-IfVbo5UY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "3775ae85-28a9-4979-cc82-8aa2e334ab8a"
      },
      "source": [
        "plt.plot(np.arange(1,epochs+1,1),train_loss)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}